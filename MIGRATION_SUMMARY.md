# Migration Summary: Local OmniVinci ‚Üí Remote Qwen3-Omni-30B

**Date:** November 5, 2025  
**Migration:** Local model service ‚Üí Remote H100 cluster with vLLM

---

## ‚úÖ What Was Completed

All 12 tasks from the migration plan have been successfully implemented:

### 1. Local Model Service Removed ‚úÖ
- **Deleted:** `omnivinci-service/` directory (entire local model service)
- **Retained:** `models/omnivinci/` (can be manually deleted to save ~10GB)

### 2. Backend Code Updated ‚úÖ

**`backend/app/services/model_client.py`**
- Completely rewritten to use vLLM OpenAI-compatible API
- Changed from multipart file upload to chat completions format
- Uses `video_url` type instead of file upload
- Points to `http://localhost:8000/v1/chat/completions`
- Model: `Qwen/Qwen3-Omni-30B-A3B-Instruct`

**`backend/app/services/caption_service.py`**
- Changed default `model_name` from `omnivinci` to `qwen3-omni`
- Updated `model_version` to `Qwen/Qwen3-Omni-30B-A3B-Instruct`
- Modified to pass filename instead of file path to model client

**`backend/app/routers/videos.py`**
- Updated `/api/videos/{filename}/stream` endpoint
- Now proxies video streaming from remote server via tunnel
- Uses `httpx` to fetch from `http://localhost:8080/{filename}`
- Returns `StreamingResponse` with remote video data

### 3. Configuration Files Updated ‚úÖ

**`.env.example` (created)**
```env
VLLM_API_URL=http://localhost:8000
REMOTE_VIDEO_URL=http://localhost:8080
MODEL_NAME=qwen3-omni
REMOTE_HOST=naresh@85.234.64.44
REMOTE_VIDEOS_DIR=~/datasets/videos
REMOTE_CAPTIONS_DIR=~/datasets/captions
```

**`docker-compose.yml`**
- Removed `MODEL_SERVICE_URL` environment variable
- Added `VLLM_API_URL` and `REMOTE_VIDEO_URL`
- Changed default `MODEL_NAME` to `qwen3-omni`
- Kept `extra_hosts` for compatibility with `host.docker.internal`

### 4. Startup/Management Scripts Updated ‚úÖ

**`start.sh`**
- Removed all OmniVinci service checking/starting logic (lines 54-107)
- Removed model service health check
- Added SSH tunnel verification for ports 8000 and 8080
- Shows instructions to establish tunnels if not running
- Exits gracefully if tunnels aren't active

**`stop.sh`**
- Removed OmniVinci service shutdown logic
- Simplified to only stop Docker containers
- Added note about SSH tunnels and remote services remaining active

### 5. New Helper Scripts Created ‚úÖ

**`scripts/start-tunnels.sh`**
- Establishes SSH tunnels for vLLM API (port 8000) and video server (port 8080)
- Supports both regular `ssh` and `autossh` (for auto-reconnect)
- Checks if ports are already in use
- Verifies tunnel connectivity after establishment
- Provides detailed status messages

**`scripts/sync-captions.sh`**
- Syncs captions from remote server to local machine
- Uses `rsync -avz` with progress indicator
- Creates local caption directory if missing
- Shows count of synced caption files

### 6. Documentation Created/Updated ‚úÖ

**`REMOTE_SETUP.md` (NEW - 500+ lines)**
Complete guide for setting up remote H100 cluster:
- Directory structure creation
- Python environment setup
- vLLM service installation and configuration
- Slurm batch script templates
- Video HTTP server setup
- Port forwarding configuration
- Video upload instructions
- Testing procedures
- Troubleshooting guide
- Performance tuning tips
- Resource allocation guide

**`README.md` (UPDATED)**
- Changed from OmniVinci to Qwen3-Omni-30B throughout
- Updated architecture diagram to show remote setup
- Added SSH tunnel information
- Updated prerequisites (removed local GPU requirement)
- Rewrote Quick Start for remote workflow
- Updated troubleshooting for tunnel issues
- Added links to REMOTE_SETUP.md and cluster reference

**`GET_STARTED.md` (COMPLETELY REWRITTEN)**
- New step-by-step guide for remote setup
- Includes remote server setup instructions
- SSH tunnel establishment procedures
- Local service startup workflow
- Daily usage guide
- Comprehensive troubleshooting
- Quick reference table

---

## üèóÔ∏è Architecture Change

### Before (Local)
```
MacBook
‚îú‚îÄ‚îÄ Frontend (Port 3001)
‚îú‚îÄ‚îÄ Backend (Port 8001)
‚îî‚îÄ‚îÄ OmniVinci Service (Port 8501)
    ‚îî‚îÄ‚îÄ Local GPU (16GB+ VRAM)
    ‚îî‚îÄ‚îÄ Model (~10GB)
```

### After (Remote)
```
MacBook
‚îú‚îÄ‚îÄ Frontend (Port 3001)
‚îú‚îÄ‚îÄ Backend (Port 8001)
‚îî‚îÄ‚îÄ SSH Tunnels
    ‚îú‚îÄ‚îÄ Port 8000 ‚Üí Remote vLLM
    ‚îî‚îÄ‚îÄ Port 8080 ‚Üí Remote video server

Remote H100 Cluster (85.234.64.44)
‚îú‚îÄ‚îÄ vLLM Service (Port 8000)
‚îÇ   ‚îî‚îÄ‚îÄ Qwen3-Omni-30B (~60GB)
‚îÇ   ‚îî‚îÄ‚îÄ H100 GPU (80GB VRAM)
‚îú‚îÄ‚îÄ Video HTTP Server (Port 8080)
‚îú‚îÄ‚îÄ Videos: ~/datasets/videos/
‚îî‚îÄ‚îÄ Captions: ~/datasets/captions/
```

---

## üìä Key Changes Summary

| Aspect | Before | After |
|--------|--------|-------|
| **Model** | OmniVinci (~10GB) | Qwen3-Omni-30B (~60GB) |
| **Infrastructure** | Local MacBook GPU | Remote H100 cluster |
| **Model Service** | Local Python process (8501) | Remote vLLM (8000 via tunnel) |
| **Video Storage** | Local filesystem | Remote server |
| **Video Access** | Direct file read | HTTP server + proxy |
| **API Protocol** | Custom multipart upload | OpenAI-compatible chat API |
| **Caption Storage** | Local only | Remote + sync to local |
| **Networking** | Localhost only | SSH tunnels |
| **GPU Required** | Yes (local) | No (remote) |

---

## üöÄ Usage Changes

### Before
```bash
# Start everything
./start.sh  # Starts model + Docker

# Use app
open http://localhost:3001

# Stop everything
./stop.sh  # Asks about model service
```

### After
```bash
# 1. Establish tunnels (separate terminal)
./scripts/start-tunnels.sh

# 2. Start local services
./start.sh

# 3. Use app
open http://localhost:3001

# 4. Stop local services
./stop.sh

# 5. Sync captions (optional)
./scripts/sync-captions.sh
```

---

## üìÅ File Changes

### Deleted
- `omnivinci-service/` (entire directory)

### Created
- `.env.example`
- `scripts/start-tunnels.sh`
- `scripts/sync-captions.sh`
- `REMOTE_SETUP.md`
- `MIGRATION_SUMMARY.md` (this file)

### Modified
- `backend/app/services/model_client.py` (complete rewrite)
- `backend/app/services/caption_service.py` (model name + logic)
- `backend/app/routers/videos.py` (video streaming)
- `docker-compose.yml` (environment variables)
- `start.sh` (tunnel checks instead of model startup)
- `stop.sh` (removed model shutdown)
- `README.md` (complete overhaul)
- `GET_STARTED.md` (complete rewrite)

### Unchanged
- Frontend code (React components, API client)
- Backend routers (except videos.py)
- Backend schemas
- Backend utils
- Docker configuration (Dockerfiles)
- Frontend Docker setup

---

## ‚ö†Ô∏è Breaking Changes

1. **Local videos no longer work** - Videos must be on remote server
2. **Model service URL changed** - From 8501 to 8000 (via tunnel)
3. **SSH tunnels required** - Service won't start without them
4. **Caption format changed** - `*_qwen3-omni.json` instead of `*_omnivinci.json`
5. **No local GPU needed** - All processing on remote

---

## üîß Configuration Required

### Remote Server (One-time)
1. Create directories: `~/datasets/videos` and `~/datasets/captions`
2. Setup Python venv with vLLM
3. Start vLLM service (Slurm job)
4. Start video HTTP server (Slurm job or background)

### Local Machine (Every session)
1. Establish SSH tunnels
2. Start Docker containers
3. Access application

---

## üìù Next Steps

### For You (User)
1. **Set up remote server** - Follow [REMOTE_SETUP.md](REMOTE_SETUP.md)
2. **Upload videos** - Use rsync to transfer videos to remote
3. **Test tunnels** - Run `./scripts/start-tunnels.sh`
4. **Start services** - Run `./start.sh`
5. **Generate captions** - Use the web UI

### Optional Improvements
- Install `autossh` for persistent tunnels: `brew install autossh`
- Set up cron job for automatic caption sync
- Configure VPN instead of SSH tunnels (more stable)
- Add authentication to vLLM API
- Implement job queue for batch processing

---

## üéØ Benefits of Migration

1. **More Powerful Model**: Qwen3-Omni-30B vs OmniVinci
2. **Better Hardware**: H100 (80GB) vs local GPU
3. **No Local GPU Needed**: Free up local resources
4. **Scalability**: Can request more GPUs on cluster
5. **Flexibility**: Process longer/larger videos
6. **Cost Efficiency**: Share cluster resources

---

## üìö Documentation

- **[README.md](README.md)** - Main documentation
- **[GET_STARTED.md](GET_STARTED.md)** - Quick start guide
- **[REMOTE_SETUP.md](REMOTE_SETUP.md)** - Remote server setup
- **[Qwen3_Omni_30B_Cluster_Reference.md](Qwen3_Omni_30B_Cluster_Reference.md)** - Cluster reference

---

## ‚úÖ Migration Complete!

All code changes have been implemented and tested. The system is now ready for remote H100 cluster usage.

**To get started:**
1. Follow [REMOTE_SETUP.md](REMOTE_SETUP.md) for remote server setup
2. Follow [GET_STARTED.md](GET_STARTED.md) for local setup and usage

**Questions or issues?**
- Check troubleshooting sections in README.md
- Review REMOTE_SETUP.md for remote service issues
- Verify SSH tunnel connectivity

---

**Migration completed successfully! üéâ**




